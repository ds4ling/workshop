<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistics for Linguists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joseph V. Casillas, PhD" />
    <script src="assets/header-attrs/header-attrs.js"></script>
    <link href="assets/remark-css/hygge.css" rel="stylesheet" />
    <link href="assets/remark-css/rutgers.css" rel="stylesheet" />
    <link href="assets/remark-css/rutgers-fonts.css" rel="stylesheet" />
    <link href="assets/tile-view/tile-view.css" rel="stylesheet" />
    <script src="assets/tile-view/tile-view.js"></script>
    <link href="assets/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="assets/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="assets/panelset/panelset.css" rel="stylesheet" />
    <script src="assets/panelset/panelset.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"xe893e8757bf42e3b7a47f7f840cc701","expires":14}</script>
    <script src="assets/himalaya/himalaya.js"></script>
    <script src="assets/js-cookie/js.cookie.js"></script>
    <link href="assets/editable/editable.css" rel="stylesheet" />
    <script src="assets/editable/editable.js"></script>
    <script src="assets/xaringanExtra-webcam/webcam.js"></script>
    <script id="xaringanExtra-webcam-options" type="application/json">{"width":"200","height":"200","margin":"1em"}</script>
    <script src="assets/kePrint/kePrint.js"></script>
    <link href="assets/lightable/lightable.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Statistics for Linguists
]
.subtitle[
## Day 1 - The general linear model
]
.author[
### Joseph V. Casillas, PhD
]
.date[
### Rutgers University</br>Last update: 2022-11-29
]

---













class: inverse, middle

&lt;blockquote align='center' class="twitter-tweet" data-lang="de"&gt;
&lt;a href="https://twitter.com/tres_gonzaga/status/910822470927523840"&gt;&lt;/a&gt;
&lt;/blockquote&gt;

---
class: title-slide-section-grey, middle

# The general linear model

---
layout: true

# A quick review

---

### Classical MRC

- In classical Multiple Regression/Correlation (MRC) predictors are 
continuous variables

&lt;p&gt;&lt;/p&gt;

- Recall
  - Darwinian theory predicted continuous variation in traits
  - Galton and Pearson created a model in which continuous variables were used
  - Discontinuous (categorical) variables were not considered

&lt;p&gt;&lt;/p&gt;

- In real life we do run into dichotomous/discontinuous variables

- Or... if an entire experimental group gets one treatment and another group 
gets a different treatment, this is also dichotomous/discontinuous

---

### Classical ANOVA

- Classical Analysis of Variance (ANOVA) assumes that all predictors are 
discontinuous variables

- ANOVA methods are often abused by forcing continuous variables into 
discontinuous form (this can reduce your statistical power by as much as 50%)

---

### Both types of variables (continuous and categorical) exist in the real world

.pull-left[

### Categorical

- Smoker/Non-smoker

- Native speaker/L2 learner

- Voiced/voiceless segment

- Stressed/Unstressed syllable

- Etc.

]

.pull-right[

### Continuous

- Age

- Weight

- Amount of exercise

- Miles per gallon

- VOT

]

---

### The modern GLM

#### Modern GLM includes both MRC and ANOVA:

- MRC predictors are all continuous

- ANOVA predictors are all discontinuous/categorical

- But both MRC and ANOVA are both part of the same big thing: the GLM

---

### A unified model

- Before ANOVA and MRC were unified, we could not account for both levels of 
measurement within the same model

- The modern GLM can accommodate any combination of categorical and continuous 
variables

- This was not known until 1968 (Cohen, 1968)

- So now we can construct mixed models with both categorical and continuous 
variables

---
layout: true

# A brief history

---

### Remember...

.pull-left[

#### Aristotle

- Categories are inherent in the individual
- Individuals need to have some identifiable, visible feature to be classified
- It is the possession of certain observable features that puts individuals in 
categories

]

--

.pull-right[

#### Plato

- Categories are God-given
- Things possess an essence of a type
- Observable features are not reliable because they are based on sense 
perceptions

]

--

&lt;/br&gt;

#### .RUred[To Aristotle the individual was ultimate reality, but to Plato the individual was an imperfect reflection of the perfect category or its “ideal type” (= eidolon)]

---
background-image: url(https://theancientwebgreece.files.wordpress.com/2017/09/bc284-bob5_just2.jpg?w=337&amp;zoom=2)
background-size: contain
background-position: 100%

### How unification occurred

.pull-left[

- Platonic and Aristotelian philosophy came back into European culture 
(the Essentialists and the Nominalists in Scholastic Philosophy)

- Nominalists assert that groups are mental constructs (not like Plato's 
god-given groups)

- This kind of Aristotelian reasoning was revived in statistics by Jacob Cohen 
in 1968

- The solution to unifying MRC and ANOVA: .RUred[Dummy Variable Coding]

]

---

### The two disciplines

Lee Cronbach (1957) *Two Disciplines of Scientific Psychology*

.pull-left[

#### Differential Psychology&lt;/br&gt;(Galton and Pearson) 

- Influenced by Darwinian thinking

- Based on individual differences

- This led to the development of MRC

]

--

.pull-right[

#### Experimental Psychology&lt;/br&gt;(Fechner, Weber, and Wundt) 

- Relied more on typological approach

- Categorically distinct groupings to design and carry out experiments

- This led to the development of ANOVA (Fisher)

]

---

### The two disciplines

#### MRC and ANOVA are both part of the same GLM model but, over time, they began to diverge from each other:

- We need them both, but historically this separation occurred

#### Fisher (experimentalists) versus Pearson (observationalists):

- A personal/family feud existed between them
- Pearson was principle figure in stats, criticized Fisher's use of chi-square 
test in an old paper
- Thought that Fisher had done a disservice to statistics (see Lenhard, 2006)
- Both angry, held grudges

---
background-image: url(https://geneonline.news/wp-content/uploads/2016/02/Ronald-Fisher-from-Royal-Society-e1455600445283-1024x850.jpg)
background-size: 400px
background-position: 90%

### Sir Ronald Aylmer Fisher

.pull-left[

- Did agricultural experiments with plants at Rothamsted Experimental Station at Harpenden, Hertfordshire, England (Studies in Crop Variation, 1919)

- Actually did controlled experiments

- Unlike the other differential psychologists who just did observational studies

- Did not have a Platonist ideology and understood individual differences

]

---
background-image: url(https://geneonline.news/wp-content/uploads/2016/02/Ronald-Fisher-from-Royal-Society-e1455600445283-1024x850.jpg)
background-size: 400px
background-position: 90%

### Sir Ronald Aylmer Fisher

#### Invented ANOVA to support the experimental method:

.pull-left[

- Randomly selected groups will differ by some amount due to individual 
differences among members of the group

- However, experimental groups should be different beyond these random 
individual differences

- He didn’t want to just assume the groups were different

- He wanted to show the variance between groups was greater than the variance 
within groups

]

---

### Fisher's method

- Fisher's used random assignment, not random sampling 

- Random Sampling: any individual in the population has an equal chance of 
being in the sample

- Random Assignment:
  - You randomly assign each subject to a treatment group or control group
  - You create 2 or more groups that will be subjected to 2 or more different 
  treatments
  - This is important to show that differences between treatment groups is 
  greater than chance

--

- Your sample might not be random, but your assignment to experimental groups 
should be random
- This procedure has nothing to do with representing the original population:
  - Just with how you randomly assign individuals into treatment groups from 
  the basic sample you are working with
  - Each subject must have an equal chance to get into either of the treatment 
  groups

---

### Fisher's method

- Using the central limit theorem, we know how the distribution of randomly 
selected group means will differ from the mean of the entire population

- If the treatment worked, the results should be greater than what would be 
expected by chance (meaning sampling of individuals)

- If treatment did not work, the results would not be greater than what is 
expected by chance (meaning sampling of individuals)

--

- This is what the F-ratio (for Fisher) is all about

- The numerator in the F-ratio represents the variance due to treatment effects

- The denominator is an independent estimate of the random sampling "error"

- All ANOVA assumes random assignment not necessarily random sampling

---

### Fisher's method

- This was designed for purely experimental purposes, not for naturally 
occurring phenomena, i.e., observational studies

- Should not use classical ANOVA for observational studies, only for pure 
controlled experiments

- For example, one should not use ANOVA to study naturally occurring races, 
sexes, etc., because of lack of random assignment to these groups

- For example, if you compare males with females, this is not a randomly assigned condition

--

- Researchers do this anyway

---

### Analysis of Variance (ANOVA)

- How did Sir Ronald Fisher build the ANOVA model?

- He built it from the MRC model...

---

### Summed Linear Deviations (MRC)

Sum of Linear Deviations:  

.Large[
`$$(y_{i} - \bar{y}) = (\hat{y}_{i} - \bar{y}) + (y_{i} - \hat{y}_{i})$$`
]

--

.center[
### Total Deviation = Predicted Deviation + Error Deviation
]

---

### Multiple Regression/Correlation

#### In MRC, the predicted y ( `\(\hat{y}_{i}\)` ) is the score predicted based on the regression line:

- MRC is based on having individual scores as the criterion variable (y)

- Individual continuous variables are also the predictors for y

---

### Summed Linear Deviations (ANOVA)

Sum of Linear Deviations:  

.Large[
`$$(y_{i} - \bar{y}_{G}) = (\bar{y}_{j} - \bar{y}_{G}) + (y_{i} - \bar{y}_{j})$$`
]

.center[

### Total Deviation = Predicted Deviation + Error Deviation

]

---

### Analysis of Variance

In ANOVA, you are dealing with groups:

- Still trying to predict an individual's score

- But you aren't basing your prediction on other individual scores

- You are basing it on their group status

---

### Analysis of Variance

#### What is the best prediction you can make about any individual in a group if you don’t know anything else about that individual?

- Use the mean of the group to predict individual scores

- Our predicted score ( `\(\hat{y}_{i}\)` ) now becomes our group mean ( `\(\bar{y}_{j}\)` )

- So the group mean ( `\(\bar{y}_{j}\)` ) now becomes the predicted `\(\hat{y}_{i}\)`

--

#### This is because my prediction for you (if I don’t know anything else about you) is based on your group’s mean

- The grand mean is `\(\bar{y}_{G}\)` and the group mean is `\(\bar{y}_{j}\)`

---

### Sums of Squared Deviations (MRC)

SS = Sum of squares: 

.Large[
`$$\sum (y_{i} - \bar{y})^2 = \sum (\hat{y}_{i} - \bar{y})^2 + \sum (y_{i} - \hat{y}_{i})^2$$`
]

--

.Large[
`$$SS_{Total} = SS_{Predicted} + SS_{Error}$$`
]

--

.Large[
|                  |     |                                  |
| :--------------- | :-: | :------------------------------- |
| `\(SS_{Total}\)`     |  =  | `\(\sum (y_{i} - \bar{y})^2\)`       |
| `\(SS_{Predicted}\)` |  =  | `\(\sum (\hat{y}_{i} - \bar{y})^2\)` |
| `\(SS_{Error}\)`     |  =  | `\(\sum (y_{i} - \hat{y}_{i})^2\)`   |
]

---

### Sums of Squared Deviations (ANOVA)

SS = Sum of squares: 

.Large[
`$$\sum (y_{i} - \bar{y}_{G})^2 = \sum (\bar{y}_{j} - \bar{y}_{G})^2 + \sum (y_{i} - \bar{y}_{j})^2$$`
]

--

.Large[
`$$SS_{Total} = SS_{Predicted} + SS_{Error}$$`
]

--

.Large[
|                  |     |                                      |
| :--------------- | :-: | :----------------------------------- |
| `\(SS_{Total}\)`     |  =  | `\(\sum (y_{i} - \bar{y}_{G})^2\)`       |
| `\(SS_{Predicted}\)` |  =  | `\(\sum (\bar{y}_{j} - \bar{y}_{G})^2\)` |
| `\(SS_{Error}\)`     |  =  | `\(\sum (y_{i} - \bar{y}_{j})^2\)`       |
]

---

### Squared Multiple Correlation Coefficient (MRC)

.Large[
`$$R^2 = \frac{\sum (\hat{y}_{i} - \bar{y})^2} {\sum (y_{i} - \bar{y})^2}$$`

`$$R^2 = \frac{SS_{Predicted}} {SS_{Total}}$$`
]

Coefficient of determination  
Proportion of Variance Explained

---

### Squared Multiple Correlation Coefficient (ANOVA)

.Large[
`$$R^2 = \frac{\sum (\hat{y}_{j} - \bar{y}_{G})^2} {\sum (y_{i} - \bar{y}_{G})^2}$$`

`$$R^2 = \frac{SS_{Predicted}} {SS_{Total}}$$`
]

Coefficient of determination  
Proportion of Variance Explained

---

### Mean Squared Deviations (MRC)

MS = Mean Squares (Variances):

.Large[
|                  |     |                                                     |
| :--------------- | :-: | :-------------------------------------------------- |
| `\(MS_{Total}\)`     |  =  | `\(\frac{\sum (y_{i} - \bar{y})^2} {(n - 1)}\)`         |
| `\(MS_{Predicted}\)` |  =  | `\(\frac{\sum (\hat{y}_{i} - \bar{y})^2} {(k)}\)`       |
| `\(MS_{Error}\)`     |  =  | `\(\frac{\sum (y_{i} - \hat{y}_{i})^2} {(n - k - 1)}\)` |

`$$F_{(k), (n-k-1)} = \frac{\sum (\hat{y}_{i} - \bar{y})^2 / (k)}
                          {\sum (y_{i} - \hat{y}_{i})^2 / (n - k - 1)}$$`
]

---

### Mean Squared Deviations (ANOVA)

MS = Mean Squares (Variances):

.Large[
|                  |     |                                                |
| :--------------- | :-: | :--------------------------------------------- |
| `\(MS_{Total}\)`     |  =  | `\(\sum (y_{i} - \bar{y}_{G})^2 / (n - 1)\)`       |
| `\(MS_{Predicted}\)` |  =  | `\(\sum (\bar{y}_{j} - \bar{y}_{G})^2 / (g - 1)\)` |
| `\(MS_{Error}\)`     |  =  | `\(\sum (y_{i} - \bar{y}_{j})^2 / (n - g)\)`       |

`$$F_{(g-1), (n-g)} = \frac{\sum (\bar{y}_{j} - \bar{y}_{G})^2 / (g - 1)}
                          {\sum (y_{i} - \hat{y}_{j})^2 / (n - g)}$$`
]

---

### Mean Squared Deviations (MRC/ANOVA)

MS = Mean Squares (Variances):

.Large[
|                  |     |                                   |
| :--------------- | :-: | :-------------------------------- |
| `\(MS_{Total}\)`     |  =  | `\(SS_{Total} / df_{Total}\)`         |
| `\(MS_{Predicted}\)` |  =  | `\(SS_{Predicted} / df_{Predicted}\)` |
| `\(MS_{Error}\)`     |  =  | `\(SS_{Error} / df_{Error}\)`         |

`$$F-ratio = \frac{MS_{Predicted}} {MS_{Error}}$$`
]

---

### Degrees of Freedom

.pull-left[

### MRC

.Large[
|                  |     |           |
| :--------------- | :-: | :-------- |
| `\(df_{Total}\)`     |  =  | n - 1     |
| `\(df_{Predicted}\)` |  =  | k         |
| `\(df_{Error}\)`     |  =  | n - k - 1 |

`$$df_{Total} = df_{Predicted} + df_{Error}$$`
]
]

.pull-right[

### ANOVA

.Large[
|                  |     |       |
| :--------------- | :-: | :---- |
| `\(df_{Total}\)`     |  =  | n - 1 |
| `\(df_{Predicted}\)` |  =  | g - 1 |
| `\(df_{Error}\)`     |  =  | n - g |

`$$df_{Total} = df_{Predicted} + df_{Error}$$`
]
]

---

### Equivalences

.Large[
| MRC              |     | ANOVA      |
| :--------------- | :-: | :--------- |
| .white[.]        |     |            |
| `\(SS_{Predicted}\)` |  =  | `\(SS_{BG}\)`  |
| `\(SS_{Error}\)`     |  =  | `\(SS_{WG}\)`  |
| .white[.]        |     |            |
| `\(MS_{Predicted}\)` |  =  | `\(MS_{BG}\)`  |
| `\(MS_{Error}\)`     |  =  |  `\(MS_{WG}\)` |
| .white[.]        |     |            |
| k                |  =  | g - 1      |
| n - k - 1        |  =  | n - g      |
]

---

### Equivalences (MRC/ANOVA)

- ANOVA is just MRC where the predictors are categorical variables

--

- Predictions are based entirely on the group status of individuals

--

- The criterion variables are still in continuous

--

&lt;p&gt;&lt;/p&gt;

- In a controlled experiment, these are orthogonal variables:
  - Conclusions may not be theoretically valid if you are using naturally 
  occurring groups
  - But the math will still work

&lt;p&gt;&lt;/p&gt;

- The strength of ANOVA is based on proper experimental design

---

### The Logic of the F-Ratio

- If we use random assignment, the means of the groups should only differ 
by random chance (individual differences) and nothing else

- However, if we introduce an effective treatment, then the means will differ 
by random chance *plus* the effects of the treatment

- But if the treatment didn’t work, then the means will only differ by random 
chance

- This is the denominator of the F-ratio!

---
background-image: url(https://memegenerator.net/img/instances/81519428/hey-girl-whats-your-number.jpg), url(https://memegenerator.net/img/instances/81519424/hey-girl-whats-your-type.jpg)
background-position: 20% 60%, 83% 60%
background-size: 300px, 300px

.pull-left[

### .center[MRC]

]

.pull-right[

### .center[ANOVA]

]

--

.footnote[So how did unification occur?]

---
layout: true

# The General Linear Model

---
background-image: url(https://jamanetwork.com/data/Journals/PSYCH/926697/m_yot8401f1.png)
background-position: 90%
background-size: 300px

### Jacob Cohen (1923-1998)

- Statistician and Psychologist
  - Best known for his work on statistical power and effect size
  - Helped lay foundations for meta-analysis
  - Gave his name to Cohen's kappa and Cohen's d
- Made a breakthrough in how to create the modern GLM:
  - Introduced Dummy Variable method of coding groups
  - Makes MRC do ANOVA!

&lt;p&gt;&lt;/p&gt;

- (1968) Multiple regression as a general data analytic system
- (1969) Statistical Power Analysis for the Behavioral Sciences
- (1975) Applied Multiple Regression/Correlation Analysis for  
the Behavioral Sciences
- (1990) Things I have learned (so far) (1994) The earth is  
round (p&lt;.05)

---

### Dummy variables

- This was based upon the idea that category membership can be considered an 
individual characteristic

&lt;p&gt;&lt;/p&gt;

- Your category membership is part of who you are
  - You may have other individual differences in addition to your category 
  membership
  - But membership is also an individual trait!

&lt;p&gt;&lt;/p&gt;

- This was Aristotelian/Nominalist, not Platonic/Essentialist typological 
thinking!

- Aristotle said that categories inhere in the individual and that the 
individual is the ultimate reality

---

### Dummy variables

- In a sense Cohen took an Aristotelian approach in order to include both MRC 
and ANOVA within one *general* linear model

- You score each person by a dummy variable to say whether they have a feature 
that defines their group membership or not

- Dummy variable coding is just binary coding of whether you have or do not 
have trait

- Dummy variable coding will make a regular regression equation do an ANOVA

- *Multiple Regression As A General Data Analytic Method* was the name of 
Cohen's seminal article

---

### The "GLM" Compromise

- The way he told the story (in 1968), MRC had seemingly “eaten” ANOVA:
  - MRC included ANOVA
  - ANOVA was just a “special case” of MRC

&lt;p&gt;&lt;/p&gt;

- This is mathematically correct, but it upset the ANOVA guys, so they instead 
called the whole superordinate category the GLM to make everyone happy:
  - GLM with continuous predictors = MRC
  - GLM with categorical predictors = ANOVA
- If you have all of one or all of another you can call it either classical 
ANOVA or a classical MRC
- But if not, you call it a "Mixed GLM"

&lt;p&gt;&lt;/p&gt;

- How did Jacob Cohen accomplish this feat?
  - He didn’t actually cite Aristotle
  - He just did the math...

---
layout: false
class: title-slide-section-grey, middle

# Dummy variables

---
layout: true

# Dummy variables

---

### The Multiple Regression Equation

.Large[
`$$\hat{y}_{i} = a + b_{1}x_{1} + b_{2}x_{2} + b_{3}x_{3} ...$$`
]

--

.Large[

What if the predictors are dummy variables?

.center[
|         |     |         |     |        |
| :-----: | :-: | :-----: | :-: | :----: |
| `\(x_{1}\)` |  =  | `\(d_{1}\)` |  =  | 0 or 1 |
| `\(x_{2}\)` |  =  | `\(d_{2}\)` |  =  | 0 or 1 |
| `\(x_{3}\)` |  =  | `\(d_{3}\)` |  =  | 0 or 1 |
]
]

---

### The Dummy Variable Equation

.large[
A single dummy variable: 
]

.Large[
`$$\hat{y}_{i} = a + b_{1}d_{1}$$`
]

--

.large[

Evaluating the function: 

|                       |             |
| :-------------------: | :---------- |
| `\(\hat{y}_{(d1=1)} =\)`  | `\(a + b_{1}\)` |
| .white[.]             |             |
| `\(\hat{y}_{(d1=0)} =\)`  | `\(a\)`         |
]

---

### Evaluating the function

.Large[
|                                                   |
| :------------------------------------------------ |
| `\(\bar{y}_{(d1=1)} = \hat{y}_{(d1=1)} = a + b_{1}\)` |
| .white[.]                                         |
| `\(\bar{y}_{(d1=0)} = \hat{y}_{(d1=0)} = a\)`         |
]

--

### Which implies that...

.Large[
`$$b_{1} = (\bar{y}_{(d1=1)} - \bar{y}_{(d1=0)})$$`
]

---

.Large[

### You get j-1 dummies

| Groups | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |
| :----- | :-----------: | :-----------: | :-----------: |
| A      | 0             | 0             | 0             |
| B      | 1             | 0             | 0             |
| C      | 0             | 1             | 0             |
| D      | 0             | 0             | 1             |

]

---

### One level is taken as the *reference* or *baseline*. This is the intercept.

.Large[

| Groups | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |                  |
| :----- | :-----------: | :-----------: | :-----------: | :--------------- |
| **A**  | **0**         | **0**         | **0**         | ⬅︎ **Intercept** |
| B      | 1             | 0             | 0             |                  |
| C      | 0             | 1             | 0             |                  |
| D      | 0             | 0             | 1             |                  |

]

---

### j-1 dummies = j-1 comparisons

.Large[

| Groups   | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |                  |
| :------- | :-----------: | :-----------: | :-----------: | :--------------- |
| **A**    | **0**         | **0**         | **0**         | ⬅︎ **Intercept** |
| .blue[B] | .blue[1]      | 0             | 0             |                  |
| C        | 0             | 1             | 0             |                  |
| D        | 0             | 0             | 1             |                  |
|          | ⬆︎&lt;/br&gt;**A**.blue[B]|         |               |                  |
]

---

### j-1 dummies = j-1 comparisons

.Large[

| Groups   | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |                  |
| :------- | :-----------: | :-----------: | :-----------: | :--------------- |
| **A**    | **0**         | **0**         | **0**         | ⬅︎ **Intercept** |
| B        | 1             | 0             | 0             |                  |
| .blue[C] | 0             | .blue[1]      | 0             |                  |
| D        | 0             | 0             | 1             |                  |
|          |               | ⬆︎&lt;/br&gt;**A**.blue[C]|         |                  |
]

---

### j-1 dummies = j-1 comparisons

.Large[

| Groups   | d&lt;sub&gt;1&lt;/sub&gt; | d&lt;sub&gt;2&lt;/sub&gt; | d&lt;sub&gt;3&lt;/sub&gt; |                  |
| :------- | :-----------: | :-----------: | :-----------: | :--------------- |
| **A**    | **0**         | **0**         | **0**         | ⬅︎ **Intercept** |
| B        | 1             | 0             | 0             |                  |
| C        | 0             | 1             | 0             |                  |
| .blue[D] | 0             | 0             | .blue[1]      |                  |
|          |               |               | ⬆︎&lt;/br&gt;**A**.blue[D] |           |
]

--

Let's see some examples...

---

### `mtcars`

&lt;img src="index_files/figure-html/mtcars_p1-1.png" width="1008" /&gt;

---


```r
lm(mpg ~ cyl, data = mtcars)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26.66 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.97 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -6.92 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.30 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;img src="index_files/figure-html/mtcars_p2-1.png" width="1008" /&gt;

---


```r
lm(mpg ~ cyl, data = mtcars)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26.664 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.972 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.437 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -6.921 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.441 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11.564 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.299 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.905 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;img src="index_files/figure-html/mtcars_p3-1.png" width="1008" /&gt;

---


```r
lm(mpg ~ cyl, data = mtcars)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 26.664 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.972 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.437 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -6.921 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.558 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.441 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8-cyl &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11.564 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.299 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.905 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;img src="index_files/figure-html/mtcars_p4-1.png" width="1008" /&gt;

---

.pull-left[

### Regression output


```r
mtcars %&gt;%
  lm(mpg ~ cyl, data = .) %&gt;% 
  summary(.)
```

&lt;table class="table table-hover" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Std. Error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; t &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: red !important;"&gt;26.66&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.97 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black !important;"&gt;-6.92&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1e-04 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black !important;"&gt;-11.56&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.30 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -8.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0e+00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

--

.pull-right[

### Means


```r
mtcars %&gt;% 
  group_by(., cyl) %&gt;% 
  summarize(., mean_mpg = mean(mpg), 
               sd_mpg = sd(mpg))
```

&lt;table class="table table-hover" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; cyl &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; mean_mpg &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; sd_mpg &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: red !important;"&gt;26.66&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.51 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black !important;"&gt;19.74&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.45 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black !important;"&gt;15.1&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.56 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

&lt;/br&gt;

- Each simple effect is an independent samples t-test
- The baseline is compared to the other two levels of the factor
--

- Notice that `6-cyl` is not compared to `8-cyl`
- We would have to change the baseline to make that comparison

---


```r
mtcars %&gt;%
  mutate(., cyl = as.factor(cyl),
*           cyl = fct_relevel(cyl, c('6', '4', '8'))) %&gt;%
  lm(mpg ~ cyl, data = .) %&gt;% 
  summary(.)
```

&lt;table class="table table-hover" style="font-size: 18px; width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Std. Error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Intercept &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: red !important;"&gt;19.74&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 16.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; cyl4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black !important;"&gt;6.92&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0001 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; cyl8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; &lt;span style="     color: black !important;"&gt;-4.64&lt;/span&gt; &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.49 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.11 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0042 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Now we have the 6-to-8 cyl comparison
- Notice how the slopes have changed

---

### Categorical and continuous predictors - mixed GLMs

- One of the benefits of doing ANOVA with MRC is that you can include different 
types of predictors in your model, i.e., **categorical** and .blue[continuous].

- Neither classical ANOVA nor classical MRC can handle combinations of these 
two predictors

- This is possible because of dummy coding

---
layout: false
class: middle

&lt;div class="figure"&gt;
&lt;img src="index_files/figure-html/age_vocab_raw-1.png" alt="Vocabulary size as a function of age." width="1008" /&gt;
&lt;p class="caption"&gt;Vocabulary size as a function of age.&lt;/p&gt;
&lt;/div&gt;

---
layout: false
class: middle

&lt;div class="figure"&gt;
&lt;img src="index_files/figure-html/age_vocab_bivar_reg-1.png" alt="Vocabulary size as a function of age." width="1008" /&gt;
&lt;p class="caption"&gt;Vocabulary size as a function of age.&lt;/p&gt;
&lt;/div&gt;

---
template: base

### Bivariate model 

.pull-left[

`$$vocab \sim age$$`




```
## 
## Call:
## lm(formula = vocab ~ ages, data = vocab_sample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6047.8 -1665.3    19.7  1865.3  6449.0 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1196.61     546.82  -2.188   0.0298 *  
## ages         1397.87      53.84  25.962   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2444 on 198 degrees of freedom
## Multiple R-squared:  0.7729,	Adjusted R-squared:  0.7718 
## F-statistic: 674.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16
```
]

.pull-right[
&lt;img src="index_files/figure-html/age_vocab_bivar_reg2-1.png" width="468" /&gt;
]

---
template: base

### Additive model 

.pull-left[

`$$vocab \sim age + reader$$`




```
## 
## Call:
## lm(formula = vocab ~ ages + reader_type, data = vocab_sample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4166.1 -1236.6    62.4  1190.2  4909.5 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         -2254.98     394.00  -5.723 3.84e-08 ***
## ages                 1338.25      38.32  34.925  &lt; 2e-16 ***
## reader_typefrequent  3474.07     246.42  14.098  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1729 on 197 degrees of freedom
## Multiple R-squared:  0.887,	Adjusted R-squared:  0.8858 
## F-statistic:   773 on 2 and 197 DF,  p-value: &lt; 2.2e-16
```
]

.pull-right[
&lt;img src="index_files/figure-html/age_additive_plot1-1.png" width="468" /&gt;
]

---

# Dummy variables

### Categorical and continuous predictors - mixed GLMs

- A mixed GLM can account for continuous and categorical predictors

- However, if the slope of two groups are different, then you must interact 
the categorical variable with the continuous one

- The interaction term constitutes a test for "homogeneity of slopes"

- These interaction terms accommodate the possible difference in slopes and 
therefore avoids a serious model misspecification

- Leaving them out would be omitting a relevant variable!

---

# Dummy variables

### Categorical and continuous predictors - mixed GLMs

- What does it mean to have an interaction between a dummy variable and a 
continuous predictor?

- Remember that with a continuous variable we get an intercept and a slope, so 
if one is interacting with a categorical variable, it means that either the 
*intercepts* or the *slopes* of both might be different for each category

- "Homogeneity of slopes" assumes that your different groups have the same 
slope for the continuous variable

---

# Dummy variables

### Categorical and continuous predictors - mixed GLMs

- The problem is that using either classical ANOVA or classical MRC (or even 
"ANCOVA", which is a combination of both) does not permit you to handle 
interactions between these types of variables

- But using Dummy or Contrast Coding Does!

- By virtue of the numerical nature of these "coded vectors", which can be 
accommodated by MRC

---
template: base

### Multiplicative model 

.pull-left[

`\(vocab \sim age + reader + age:reader\)`




```
## 
## Call:
## lm(formula = vocab ~ ages * reader_type, data = vocab_sample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3673.5 -1036.0    22.7  1027.0  3804.4 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)               -138.67     475.28  -0.292    0.771    
## ages                      1110.73      48.42  22.939  &lt; 2e-16 ***
## reader_typefrequent      -1027.18     705.63  -1.456    0.147    
## ages:reader_typefrequent   465.73      69.28   6.723  1.9e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1562 on 196 degrees of freedom
## Multiple R-squared:  0.9082,	Adjusted R-squared:  0.9067 
## F-statistic:   646 on 3 and 196 DF,  p-value: &lt; 2.2e-16
```

]

.pull-right[

&lt;img src="index_files/figure-html/age_int_plot1-1.png" width="468" /&gt;

]

---
template: base
class: middle


&lt;table style="text-align:center"&gt;&lt;tr&gt;&lt;td colspan="4" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td colspan="3"&gt;&lt;em&gt;Dependent variable:&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan="3" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td colspan="3"&gt;vocab&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;(1)&lt;/td&gt;&lt;td&gt;(2)&lt;/td&gt;&lt;td&gt;(3)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan="4" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;Constant&lt;/td&gt;&lt;td&gt;-1,196.606&lt;sup&gt;**&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;-2,254.983&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;-138.673&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;(546.816)&lt;/td&gt;&lt;td&gt;(393.996)&lt;/td&gt;&lt;td&gt;(475.279)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;ages&lt;/td&gt;&lt;td&gt;1,397.867&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;1,338.249&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;1,110.735&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;(53.842)&lt;/td&gt;&lt;td&gt;(38.318)&lt;/td&gt;&lt;td&gt;(48.421)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;reader_typefrequent&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;3,474.068&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;-1,027.182&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(246.423)&lt;/td&gt;&lt;td&gt;(705.629)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;ages:reader_typefrequent&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;465.733&lt;sup&gt;***&lt;/sup&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;(69.278)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan="4" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;Observations&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;td&gt;200&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.773&lt;/td&gt;&lt;td&gt;0.887&lt;/td&gt;&lt;td&gt;0.908&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;Adjusted R&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;&lt;td&gt;0.772&lt;/td&gt;&lt;td&gt;0.886&lt;/td&gt;&lt;td&gt;0.907&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;Residual Std. Error&lt;/td&gt;&lt;td&gt;2,444.005 (df = 198)&lt;/td&gt;&lt;td&gt;1,728.713 (df = 197)&lt;/td&gt;&lt;td&gt;1,562.329 (df = 196)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td style="text-align:left"&gt;F Statistic&lt;/td&gt;&lt;td&gt;674.050&lt;sup&gt;***&lt;/sup&gt; (df = 1; 198)&lt;/td&gt;&lt;td&gt;773.006&lt;sup&gt;***&lt;/sup&gt; (df = 2; 197)&lt;/td&gt;&lt;td&gt;646.011&lt;sup&gt;***&lt;/sup&gt; (df = 3; 196)&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan="4" style="border-bottom: 1px solid black"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align:left"&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/td&gt;&lt;td colspan="3" style="text-align:right"&gt;&lt;sup&gt;*&lt;/sup&gt;p&lt;0.1; &lt;sup&gt;**&lt;/sup&gt;p&lt;0.05; &lt;sup&gt;***&lt;/sup&gt;p&lt;0.01&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

---
template: base




```r
anova(vocab_age_null, vocab_age_mod, vocab_additive_mod, vocab_int_mod)
```

```
## Analysis of Variance Table
## 
## Model 1: vocab ~ 1
## Model 2: vocab ~ ages
## Model 3: vocab ~ ages + reader_type
## Model 4: vocab ~ ages * reader_type
##   Res.Df        RSS Df  Sum of Sq        F    Pr(&gt;F)    
## 1    199 5208897978                                     
## 2    198 1182686093  1 4026211885 1649.497 &lt; 2.2e-16 ***
## 3    197  588724259  1  593961835  243.340 &lt; 2.2e-16 ***
## 4    196  478410910  1  110313349   45.194 1.901e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

--

The vocabulary data were analyzed using a general linear model. Estimated 
vocabulary size was the criterion with *age* and *reader type* 
(frequent/average) as predictors. The *reader type* factor was dummy coded with 
average readers set as the reference group. Main effects and the *age* by 
*reader type* interaction were assessed using nested model comparisons. 
Experiment-wise alpha was set at 0.05. 

There was a main effect of age (F(1) = 1649.49, p &lt; 0.001), reader type 
(F(1) = 243.34; p &lt; 0.001), as well as an age by reader type interaction 
(F(1) = 45.19; p &lt; 0.001). The model containing the interaction provided the 
best fit of the data (R&lt;sup&gt;2&lt;/sup&gt; = 0.91). Overall, vocabulary size increased 
as a function of age. However, the size of the effect was modulated by reader 
type. Specifically, average readers showed an increase of approximately 1,111 
words +/- 48.42 se (t = 22.94, p &lt; 0.001) per year. Frequent readers showed an 
additional increase of 466 words +/- 69.28 se per year 
(1,577 words total, t = 6.72, p &lt; 0.001). 

---
class: middle, center

&lt;iframe src="https://gallery.shinyapps.io/multi_regression/" style="border:none;" height="600" width="1300"&gt;&lt;/iframe&gt;

---

### Practice

[dummy variable practice](./assets/dummies_walkthrough/dummies.zip)

---
exclude: true

(Wickham and Grolemund, 2016; Hardy, 1993b; Hardy, 1993c; Hardy, 1993d)

---
layout: false
class: title-slide-final, left

# References

[1] W. Berry and S. Feldman. "Multicollinearity". In: _Multiple
Regression in Practice_. Ed. by W. Berry and S. Feldman. Sage University
Paper Series on Quantitative Applications in the Social Sciences - 50.
Newbury Park, CA: Sage, 1985, pp. 37-50. ISBN: 9780803920545.

[2] W. Berry and S. Feldman. "Specification Error". In: _Multiple
Regression in Practice_. Ed. by W. Berry and S. Feldman. Sage University
Paper Series on Quantitative Applications in the Social Sciences - 50.
Newbury Park, CA: Sage, 1985, pp. 18-25. ISBN: 9780803920545.

[3] M. Hardy. "Assessing Group Differences in Effects". In: _Regression
with Dummy Variables_. Ed. by M. Hardy. Sage University Paper Series on
Quantitative Applications in the Social Sciences - 93. Newbury Park, CA:
Sage, 1993, pp. 29-63. ISBN: 9780803951280.

[4] M. Hardy. "Creating Dummy Variables". In: _Regression with Dummy
Variables_. Ed. by M. Hardy. Sage University Paper Series on Quantitative
Applications in the Social Sciences - 93. Newbury Park, CA: Sage, 1993,
pp. 7-17. ISBN: 9780803951280.

[5] M. Hardy. "Using Dummy Variables as Regressors". In: _Regression with
Dummy Variables_. Ed. by M. Hardy. Sage University Paper Series on
Quantitative Applications in the Social Sciences - 93. Newbury Park, CA:
Sage, 1993, pp. 18-28. ISBN: 9780803951280.

[6] M. Lewis-Beck. "Multiple Regression". In: _Applied Regression: An
Introduction_. Ed. by M. Lewis-Beck. Sage University Paper Series on
Quantitative Applications in the Social Sciences - 22. Newbury Park, CA:
Sage, 1980, pp. 47-74. ISBN: 9781483381497.

[7] L. Schroeder, D. Sjoquist, and P. Stephan. "Multiple Linear
Regression". In: _Understanding Regression Analysis: An Introductory
Guide_. Ed. by L. Schroeder, D. Sjoquist and P. Stephan. Sage University
Paper Series on Quantitative Applications in the Social Sciences - 57.
Newbury Park, CA: Sage, 1986, pp. 29-35. ISBN: 9780803927582.

[8] L. Schroeder, D. Sjoquist, and P. Stephan. "Problems and Issues of
Linear Regression". In: _Understanding Regression Analysis: An
Introductory Guide_. Ed. by L. Schroeder, D. Sjoquist and P. Stephan.
Sage University Paper Series on Quantitative Applications in the Social
Sciences - 57. Newbury Park, CA: Sage, 1986, pp. 65-80. ISBN:
9780803927582.

[9] H. Wickham and G. Grolemund. _R for Data Science: Import, Tidy,
Transform, Visualize, and Model Data_. O'Reilly Media, 2016.

Figueredo, A. J. (2013). Continuous and Categorical Predictors. *Statistical Methods in Psychological Research*.

Figueredo, A. J. (2013). The General Linear Model: ANOVA. *Statistical Methods in Psychological Research*.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="http://www.jvcasillas.com/ru_xaringan/js/ru_xaringan.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "default",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
