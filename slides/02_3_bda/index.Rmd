---
title   : 'Statistics for Linguists'
subtitle: 'Day 2 - Bayesian inference'
author  : "Joseph V. Casillas, PhD"
date    : "Rutgers University</br>Last update: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: assets
    css: ["hygge", "rutgers", "rutgers-fonts"]
    nature:
      beforeInit: ["https://www.jvcasillas.com/ru_xaringan/js/ru_xaringan.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: default
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../assets/partials/header.html"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-extra-all-the-things, echo=FALSE}
xaringanExtra::use_xaringan_extra(
  c("tile_view", "panelset", "editable"
    #"scribble", "search", "webcam"
    )
)
xaringanExtra:::use_freezeframe()
```

```{r, 'helpers', echo=FALSE, message=F, warning=F}
source(here::here("slides", "assets", "scripts", "helpers.R"))
```

```{r, load_refs, echo=FALSE, cache=FALSE, warning=F, message=F}
bib <- ReadBib(here("slides", "assets", "bib", "ds4ling_refs.bib"), check = FALSE)
ui <- "- "
```

```{r 'global_setup', echo=FALSE}
opts_chunk$set(fig.retina=2, cache=FALSE, warning=F, message=F)
```

class: title-slide-section-grey, middle

# What is probability?

---

class: title-slide-section-grey, middle

# The world according to frequentists

---
background-image: url(https://www.bellevuerarecoins.com/wp-content/uploads/bigstock-Coin-Flip-5807921.jpg)
background-size: contain
background-position: 100% 50%

# What is frequentism?

.pull-left[
- A blanket term used to refer to "classical" statistics

- Population parameters are fixed, actually exist

- Probability refers to the long-run frequency of a given event

- A sample of data is the result of one of an infinite number of exactly repeated experiments

- Data are random, result from sampling a fixed population distribution
]

???

- Frequentist statistics represent what we have been doing all semester
- The frequentist view of probability is what leads to odd definitions of statistical machinery, like confidence intervals
- This view of probability seems to be at odds with how we think/reason
  - What is the probability X candidate wins the election?

---
class: center, middle

# What if...

### There isn't one true population parameter... <br>
--
but an entire distribution of parameters, some more plausible than others

---
class: middle

```{r, mtcars-ex, echo=F}
mod_f <-  lm(mpg ~ drat, data = mtcars)
mod_b <- brm(mpg ~ drat, data = mtcars, file = here("slides", "assets", "mods", "cars_b"))

cars_post <- as_tibble(mod_b) %>% select(int = b_Intercept, b = b_drat)

plot_base <- mtcars %>% 
  ggplot(., aes(x = drat, y = mpg)) + 
    geom_point(pch = 20, color = "black") + 
    ds4ling_bw_theme(base_size = 18)

plot_f <- plot_base + 
  geom_smooth(method = lm, color = "darkred", formula = "y ~ x", size = 1.5) + 
  labs(title = "Frequentist model", subtitle = "Line of best fit")
plot_b <- plot_base + 
  geom_abline(intercept = fixef(mod_b)[1, 1], slope = fixef(mod_b)[2, 1], 
    color = "darkred", size = 1.5) + 
  labs(title = "Bayesian model", subtitle = "Most plausible line of best fit")
plot_b_20 <- plot_base + 
  geom_abline(data = sample_n(cars_post, 20), 
    aes(intercept = int, slope = b), color = "grey80", alpha = 0.5) + 
  geom_abline(intercept = fixef(mod_b)[1, 1], slope = fixef(mod_b)[2, 1], 
    color = "darkred", size = 1.5) + 
  labs(title = "Bayesian model", subtitle = "Median line +20 plausible lines")
plot_b_50 <- plot_base + 
  geom_abline(data = sample_n(cars_post, 50), 
    aes(intercept = int, slope = b), color = "grey80", alpha = 0.5) + 
  geom_abline(intercept = fixef(mod_b)[1, 1], slope = fixef(mod_b)[2, 1], 
    color = "darkred", size = 1.5) + 
  labs(title = "Bayesian model", subtitle = "Median line +50 plausible lines")
plot_b_200 <- plot_base + 
  geom_abline(data = sample_n(cars_post, 200), 
    aes(intercept = int, slope = b), color = "grey80", alpha = 0.5) + 
  geom_abline(intercept = fixef(mod_b)[1, 1], slope = fixef(mod_b)[2, 1], 
    color = "darkred", size = 1.5) + 
  labs(title = "Bayesian model", subtitle = "Median line +200 plausible lines")
```

```{r, cars-f-vs-cars-b1, echo=F, fig.width=13, fig.height=5.5}
plot_f + plot_spacer() 
```

---
class: middle
count: false

```{r, cars-f-vs-cars-b2, echo=F, fig.width=13, fig.height=5.5}
plot_f + plot_b 
```

---
class: middle

```{r, cars-b-draws1, echo=F, fig.width=13, fig.height=5.5}
plot_b_20 + plot_spacer() + plot_spacer()
```

---
class: middle
count: false

```{r, cars-b-draws2, echo=F, fig.width=13, fig.height=5.5}
plot_b_20 + plot_b_50 + plot_spacer()
```

---
class: middle
count: false

```{r, cars-b-draws3, echo=F, fig.width=13, fig.height=5.5}
plot_b_20 + plot_b_50 + plot_b_200
```

---
class: middle

```{r, cars-f-vs-cars-b-2, echo=F, fig.width=13, fig.height=5.5}
plot_f + plot_b_200
```

???

Classical: There is a single "true" line of best fit, and I'll give my best estimate of it.

Bayesian: There is a distribution of lines of fit...some more plausible than others...and I'll give you samples from that distribution.

---
background-color: black
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/rstats/memes/bayesian_bey.png)
background-size: contain

---

# Distributions of plausible outcomes

.pull-left[

- In Bayesian inference we attempt to approximate this (these) distribution(s)... we call it the **posterior distribution**

- Bayesian inference is all about the posterior

- It represents a distribution of estimates that could arise from the data

- Values that are more common (appear more often) are more plausible

- For *very* simple problems we can calculate the posterior analytically by integrating (calculus)

- For most problems (regression) we cannot calculate the posterior... so we approximate it via sampling (and calculus)

]

.pull-right[
<br>
```{r, normal-dist, echo=F, fig.width=6, fig.height=4}
ggplot(data.frame(x = c(10, 90)), aes(x = x)) +
    stat_function(fun = dnorm, n = 100, args = list(mean = 50, sd = 10),
      size = 1, color = "darkred") + 
    labs(y = "Density", x = expression(beta)) + 
    ds4ling::ds4ling_bw_theme(base_size = 18) 
```

]

---

# A trivial example

.pull-left[

### More cars

- Let's explore the `mpg` variable

- N = `r length(mtcars$mpg)`

- The range is `r range(mtcars$mpg)`. 

- The mean is `r mean(mtcars$mpg)`. 

- The SD is `r sd(mtcars$mpg)`.

- The 95% quantiles are `r quantile(mtcars$mpg, probs = c(0.025, 0.975))`

- We'll fit an intercept only model. 

]

.pull-right[

</br></br>
```{r, mtcars-mpg-plot, echo=F, fig.height=4}
ggplot(mtcars, aes(x = NA, y = mpg)) + 
  geom_point(size = 3, alpha = 0.7, color = "grey40", 
    position = position_nudge(x = 0.1)) + 
  stat_summary(fun.data = mean_sdl, geom = "pointrange", 
    fun.args = list(mult = 1), pch = 21, fill = "white", size = 2, 
    position = position_nudge(x = -0.1)) + 
  labs(title = "Distribution of mpg", 
    subtitle = "Raw data and mean +/- SD", 
    x = NULL) + 
  coord_flip() + 
  ds4ling_bw_theme(base_size = 18) + 
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
```

]

--

.pull-left[
.center[

$$mpg \sim \alpha + \epsilon$$

```lm(mpg ~ 1)```

]
]

---

# A trivial example

We'll fit an intercept only model. The mean `mpg` is `r mean(mtcars$mpg)`. 

### Frequentist model

```{r, cars-int-only-freq}
lm(mpg ~ 1, data = mtcars) %>% summary()
```

---

# A trivial example

We'll fit an intercept only model. The mean `mpg` is `r mean(mtcars$mpg)`. 

--

### Bayesian model

```{r, cars-int-only-bayes-hold, eval=F}
brm(mpg ~ 1, data = mtcars) %>% summary
```

```{r, cars-int-only-bayes, echo=F}
cars_bayes_int <- brm(mpg ~ 1, data = mtcars, 
  file = here("slides", "assets", "mods", "cars_int"))
summary(cars_bayes_int)
```

---

# Exploring the posterior

.pull-left[
```{r, cars-int-posterior, eval=F}
cp <- as_tibble(cars_bayes_int)
```

```{r, cars-int-posterior-show, echo=F}
as_tibble(cars_bayes_int) %>% select(b_Intercept)
```

- The posterior distribution is just like any other distribution of data we've seen

- We can analyze it however we want

]

--

.pull-right[

```{r, cars-post-plot, echo=F, fig.height=4.5}
cp <- as_tibble(cars_bayes_int)$b_Intercept

ggplot(tibble(mpg = cp), aes(x = mpg, y = NA)) + 
  geom_point(data = mtcars, aes(x = mpg), size = 3, alpha = 0.7, 
    color = "grey40", position = position_nudge(y = -0.1)) +
  tidybayes::stat_halfeye(pch = 21, point_fill = "white", point_size = 5) + 
  labs(title = "Distribution of mpg", 
    subtitle = "Raw data and posterior mean +/- 95% CI", 
    x = "mpg", y = NULL) + 
  ds4ling_bw_theme(base_size = 18) + 
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())
```

```{r, cars-post-play}
mean(cp); quantile(cp, probs = c(0.025, 0.975))
```

]

---
class: title-slide-section-grey, middle

# **How do we get a posterior?**

### Let's talk about probability

---
class: middle, center

.Large[
$$P(\color{green}{y})$$
]

--

.Large["the probability of .green[y]"]

--

.Large["the probability of .green[you getting an A in stats]"]

---
class: middle, center

.Large[
$$P(\color{green}{y} | \color{red}{z} )$$
]

--

.Large["the probability of .green[y] given .red[z]"]

--

.Large["the probability of .green[you getting an A in stats] given .red[you don't do the readings]"]

---

# How do we get a posterior?

<br><br><br>

.center[
.Large[
$$
P(A | B) = \frac{P(B | A) \times P(A)}{P(B)}
$$
]
]

--

Read as: 

<ru-blockquote>
The probability of A given B is equal to the probability of B given A times the probability of A divided by the probability of B
</ru-blockquote>

--

.content-box-red[
There is a good chance you never actually use this, even though most texts on Bayesian inference will start with this and base rate fallacy examples 
]

???

It tells you how to convert one conditional probability into another one.

---
background-image: url(./assets/img/bayes_theorem.gif)
background-size: 600px
background-position: 50% 40%

# How do we get a posterior?

---
background-image: url(./assets/img/bayes_theorem.png)
background-size: 600px
background-position: 50% 40%

# How do we get a posterior?

--

.footnote[
.content-box-blue[
The posterior distribution is the product of the likelihood and the prior...<br>
(divided by the normalizing constant)
]
]

???

This is much more interesting in the context of statistical modelling

In a class on Bayesian inference we would dedicate a lot of time to understanding the likelihood and the prior...

The prior is the most controversial part of Bayesian inference

---

# How do we get a posterior?

<br><br><br>

.Large[
$$
Posterior = \frac{Likelihood \times Prior}{Normalizing\ constant}
$$
]

???

Prior and posterior describe when information is obtained: what we know pre-data is our prior information, and what we learn post-data is the updated information (“posterior”).

The likelihood in the equation says how likely the data is given the model parameters. 
I think of it as fit: How well do the parameters fit the data? 
Classical regression's line of best fit is the maximum likelihood line. 
The likelihood also encompasses the data-generating process behind the model. 
For example, if we assume that the observed data is normally distributed, then we evaluate the likelihood by using the normal probability density function. 
You don't need to know what that last sentence means. 
What's important is that the likelihood contains our built-in assumptions about how the data is distributed.

---
class: middle

<iframe src="https://seeing-theory.brown.edu/bayesian-inference/index.html" style="border:none;" width="100%" height="100%">

---

# How do we get a posterior?

<br><br><br>

.Large[
$$
updated\ knowledge \propto Likelihood\ of\ data \times prior\ beliefs
$$
]

--

.footnote[
.content-box-blue[
We can think of Bayes' theorem as a principled/systematic method for updating our knowledge in light of new data.

- Update beliefs in proportion to how well the data fit those beliefs.
- Because beliefs have probabilities, we can quantify our uncertainty about them.

]
]

---

# How do we get a posterior?

<br><br><br>

.Large[
$$
P(unknown | observed) = \frac{\color{red}{P(observed | unknown)} \times P(unknown)}{P(observed)}
$$
]

???

We have some model that describes a data-generating process and we have some observed data, but we want to estimate some unknown model parameters. In that case, the formula reads like:

When I was first learning Bayes, this form was my anchor for remembering the formula. The goal is to learn about unknown quantity from data, so the left side needs to be “unknown given data”.

---
class: title-slide-section-grey, middle

# Frequentism vs. Bayesianism

---

# Frequentism vs. Bayesianism

.center[
In essence the frequentist approach assess the probability of the data given a specific hypothesis
]

.large[
$$P(data | H_{x})$$
]

<br>

--

.center[
Through Bayes formula one is able to asses the probability of a specific hypothesis given the data
]

.large[
$$P(H_{x} | data)$$
]

<br>

--

.center[
.content-box-blue[
This is what we pretty much always want in science <br>(and what many people think they are doing with frequentist statistics)
]
]

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/rstats/memes/rstats_ci_1.png)
background-size: contain

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/rstats/memes/rstats_ci_2.png)
background-size: contain

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/rstats/memes/bayesian_priors.png)
background-size: contain
background-position: 100% 50%
background-color: black

# About priors

.pull-left[
.white[
- To approximate the posterior distribution we need priors and data

- Priors = prior beliefs or assumptions

- They represent a way for us to incorporate prior experience or domain expertise into the model 

- You can set any prior you want (subjective)

- Many object to Bayesian inference because of the priors

- Counterpoint: priors force you to be explicit about your assumptions
]
]

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/rstats/memes/bayesian_likelihoods.png)
background-size: contain
background-color: black

---

# Why?

.pull-left[
### Advantages

- A more intuitive inferential framework
- Focus on distributions and uncertainty estimation instead of point estimates
- More natural interpretation of results
- Ability to handle small samples with appropriate guard against overfitting
- Natural/principled way to combine prior information with data, within a solid decision theoretical framework
- Flexible, can fit *many* models
- Multiple comparisons?
- Evidence for the null?
- Model convergence?
]

--

.pull-right[
### Disadvantages

- Conceptually difficult
- Steep learning curve
- Requires careful consideration of prior assumptions
- Frequentist probability is based on an imaginary set of experiments that you never actually carry out
- Manipulating posterior takes practice
- Less common, less accepted
- Computationally costly, slow
- There is no correct way to choose a prior
]

---
class: title-slide-section-grey, middle

# Frequentism vs. Bayesianism?

---

# Frequentism vs. Bayesianism?

<br>

.Large[
- A Bayesian estimate with flat priors is generally equivalent to a frequentist MLE

- Frequentism is not wrong, just different

- Frequentism is susceptible to QRPs

- It's about using the right tool for the job

- Do whatever you need to do answer your questions
]

---
class: title-slide-section-grey, middle

# How you do it?

---

# More sleep (but Bayesian)

### `sleepstudy` dataset

.pull-left[

- Part of `lme4` package

- Criterion = reaction time (`Reaction`)

- Predictor = \# of days of sleep deprivation (`Days`)

- 10 observations per participant

- \+2 fake participants w/ incomplete data

]

.pull-right[

```{r, ss-cleanup, echo=F}
# Convert to tibble for better printing. 
# Convert factors to strings
library("lme4")
sleepstudy <- sleepstudy %>% 
  as_tibble() %>% 
  mutate(Subject = as.character(Subject))

# Add two fake participants
df_sleep <- bind_rows(
  sleepstudy,
  tibble(Reaction = c(286, 288), Days = 0:1, Subject = "374"),
  tibble(Reaction = 245, Days = 0, Subject = "373"))
```

```{r, ss-add-subjs}
str(df_sleep)
head(df_sleep)
```

]

---
class: middle

```{r, ss-plot0, fig.width=14, fig.height=8.5, echo=F}
df_sleep %>% 
  ggplot(., aes(x = Days, y = Reaction)) + 
    geom_point(pch = 21, aes(fill = Subject)) + 
    scale_x_continuous(breaks = 0:4 * 2) + 
    ds4ling_bw_theme(base_size = 18, base_family = "Times")
```

---
class: middle

```{r, ss-plot-mod, echo=F}

xlab <- "Days of sleep deprivation"
ylab <- "Average reaction time (ms)"

# No pooling
df_no_pooling <- lmList(Reaction ~ Days | Subject, df_sleep) %>% 
  coef() %>% 
  tibble::rownames_to_column("Subject") %>% 
  rename(Intercept = `(Intercept)`, Slope_Days = Days) %>% 
  mutate(Model = "No pooling") %>% 
  dplyr::filter(Subject != "373")

m_pooled <- lm(Reaction ~ Days, df_sleep) 

# Repeat the intercept and slope terms for each participant
df_pooled <- tibble(
  Model = "Complete pooling",
  Subject = unique(df_sleep$Subject),
  Intercept = coef(m_pooled)[1], 
  Slope_Days = coef(m_pooled)[2]
)

# Fit partial pooling model
m <- lmer(Reaction ~ 1 + Days + (1 + Days | Subject), df_sleep)

# Make a dataframe with the fitted effects
df_partial_pooling <- coef(m)[["Subject"]] %>% 
  tibble::rownames_to_column("Subject") %>% 
  as_tibble() %>% 
  rename(Intercept = `(Intercept)`, Slope_Days = Days) %>% 
  mutate(Model = "Partial pooling")

df_models <- bind_rows(
  df_pooled, df_no_pooling, df_partial_pooling) %>%
  left_join(df_sleep, by = "Subject")


p_model_comparison <- ggplot(df_models) + 
  aes(x = Days, y = Reaction) + 
  geom_abline(
    aes(intercept = Intercept, slope = Slope_Days, color = Model),
    size = .75
  ) + 
  geom_point() +
  facet_wrap("Subject") +
  labs(x = xlab, y = ylab, 
    title = "Comparison", 
    subtitle = "No pooling vs. Complete pooling vs. Partial pooling") + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  scale_color_brewer(palette = "Set1") + 
  ds4ling_bw_theme(base_size = 18, base_family = "Times") + 
  theme(legend.position = "bottom", legend.justification = "left")

summary(m)
```

---
class: middle

```{r, ss-plot1, fig.width=14, fig.height=8.5, echo=F}
p_model_comparison
```

---

# Bayesian version

### Fitting the model

.center[
.Large[
$$
\begin{aligned}
Reaction_{ij} & \sim normal(\mu, \sigma) \\
\mu & = \alpha_{ij} + \beta * Days_{ij} \\
\alpha & \sim normal(0, 1) \\
\beta & \sim normal(0, 1) \\
\sigma & \sim normal(0, 1)
\end{aligned}
$$
]
]

--

</br>
.center[
.Large[
```
brm(Reaction ~ 1 + Days + (1 + Days | Subject), data = sleepstudy)
```
]
]

---
class: middle

```{r, sleep-brms-mod, echo=F}
sleep_mod <- readRDS(here("slides", "assets", "mods", "sleep_mod.rds"))

# Get a dataframe: One row per posterior sample
df_posterior <- sleep_mod %>% 
  as_tibble()

summary(sleep_mod)
```

---

```{r, sleep-draws-main, fig.width=14, fig.height=8.5, echo=F}

df_sleep %>% 
  ggplot(., aes(x = Days, y = Reaction)) + 
    scale_x_continuous(breaks = 0:4 * 2) + 
    geom_abline(data = sample_n(df_posterior, 250), color = "grey80", 
      alpha = 0.2, size = 1, 
      aes(intercept = b_Intercept, slope = b_Days)) + 
    geom_point(pch = 21, fill = "black", size = 5, color = "white") + 
    geom_abline(color = "white", size = 3.5, 
      intercept = fixef(sleep_mod)[1, 1], slope = fixef(sleep_mod)[2, 1]) +
    geom_abline(color = "darkred", size = 1.5, 
      intercept = fixef(sleep_mod)[1, 1], slope = fixef(sleep_mod)[2, 1]) +
    ds4ling_bw_theme(base_size = 18, base_family = "Times")

```

---

```{r, sleep-draws-forest, fig.width=14, fig.height=8.5, echo=F}
df_posterior %>% 
  select(Intercept = b_Intercept, Days = b_Days) %>% 
  pivot_longer(cols = everything(), names_to = "parameter", 
    values_to = "estimate") %>% 
  ggplot(., aes(x = estimate, y = parameter)) + 
    geom_vline(xintercept = 0, lty = 3) + 
    tidybayes::stat_halfeye(pch = 21, point_fill = "white", point_size = 3, 
      .width = c(0.66, 0.95)) + 
    labs(title = "BDA", 
      subtitle = "Forest plot of population estimates", 
      caption = "Posterior means +/- 66% and 95% CI", 
      y = "Parameter", x = "Estimate") + 
    ds4ling_bw_theme(base_size = 18)
```

---
background-color: black

```{r, sleep-draws-solar-system, fig.width=14, fig.height=8.5, echo=F}
sleep_solar <- ggplot(df_posterior) + 
  aes(x = b_Intercept, y = b_Days) + 
  stat_density_2d(geom = "raster", aes(fill = ..density..),
    n = 200, contour = FALSE) +
  scale_fill_viridis_c(option = "A") + 
  labs(title = "Where's the average intercept and slope?", 
   x = "Estimate for average intercept", 
   y = "Estimate for average slope") + 
  ggdark::dark_theme_gray(base_size = 14) 

sleep_solar

#library(rayshader)
#plot_gg(sleep_solar, width = 5, height = 5, multicore = TRUE, scale = 250,
#        zoom = 0.7, theta = 10, phi = 30, windowsize = c(800, 800))
```

---

```{r, sleep-draws-spaghetti, fig.width=14, fig.height=8.5, echo=F}
samples <- df_posterior %>% 
  mutate_at(
    .vars = vars(matches("Intercept\\]")), 
    .funs = ~ . + df_posterior$b_Intercept
  ) %>% 
  mutate_at(
    .vars = vars(matches("Days\\]")), 
    .funs = ~ . + df_posterior$b_Days
  ) %>% 
  select(starts_with("r_Subject")) %>% 
  pivot_longer(cols = everything(), names_to = c("Subject", ".value"), 
    names_sep = ",") %>% 
  transmute(Intercept = `Intercept]`, Days = `Days]`, 
    Subject = stringr::str_remove(Subject, "r_Subject\\[")) 

df_sleep %>% 
  ggplot(., aes(x = Days, y = Reaction)) + 
    facet_wrap(~ Subject) + 
    geom_point(color = "black") + 
    geom_abline(
      data = samples %>% group_by(Subject) %>% sample_n(., size = 50), 
      aes(intercept = Intercept, slope = Days), 
      color = "darkred", alpha = 0.1
      ) + 
  labs(x = xlab, y = ylab, 
    title = "BDA", 
    subtitle = "50 draws of plausible lines of best fit for each subject") + 
  scale_x_continuous(breaks = 0:4 * 2) + 
  ds4ling_bw_theme(base_size = 18, base_family = "Times") + 
  theme(legend.position = "bottom", legend.justification = "left")

```

---
background-color: black

```{r, sleep-multiverse, fig.width=14, fig.height=8.5, echo=F}
samples %>% 
  ggplot(., aes(x = Intercept, y = Days)) + 
    facet_wrap(~ Subject) + 
    #stat_density_2d(aes(fill = stat(level)), geom = "polygon",
    #  show.legend = F,contour_var = "ndensity") + 
    stat_density_2d(geom = "raster", aes(fill = ..density..),
    n = 100, contour = FALSE, show.legend = F, contour_var = "ndensity") + 
    scale_fill_viridis_c(option = "A") + 
    ggdark::dark_theme_gray(base_size = 14) 
```

---
class: middle
background-color: black

```{r, echo=F, fig.width=13.5, fig.height=5}
data <- tibble(
  age = c(38, 45, 52, 61, 80, 74), 
  prop = c(0.146, 0.241, 0.571, 0.745, 0.843, 0.738)
)

inv_logit <- function(x) 1 / (1 + exp(-x))

model_formula <- bf(
  # Logistic curve
  prop ~ inv_logit(asymlogit) * inv(1 + exp((mid - age) * exp(scale))),
  # Each term in the logistic equation gets a linear model
  asymlogit ~ 1,
  mid ~ 1,
  scale ~ 1,
  # Precision
  phi ~ 1,
  # This is a nonlinear Beta regression model
  nl = TRUE, 
  family = Beta(link = identity)
)

prior_fixef <- c(
  # Point of steepest growth is age 4 plus/minus 2 years
  prior(normal(48, 12), nlpar = "mid", coef = "Intercept"),
  prior(normal(1.25, .75), nlpar = "asymlogit", coef = "Intercept"),
  prior(normal(-2, 1), nlpar = "scale", coef = "Intercept")
)

prior_phi <- c(
  prior(normal(2, 1), dpar = "phi", class = "Intercept")
)

fit_prior <- brm(
  model_formula,
  data = data,
  prior = c(prior_fixef, prior_phi),
  iter = 2000,
  chains = 4,
  sample_prior = "only", 
  cores = 1,
  control = list(adapt_delta = 0.9, max_treedepth = 15), 
  file = here("slides", "assets", "mods", "log_curve_prior")
)

draws_prior <- data %>%
  tidyr::expand(age = 0:100) %>%
  tidybayes::add_fitted_draws(fit_prior, n = 100)

p1 <- ggplot(draws_prior) +
  aes(x = age, y = .value) +
  geom_line(aes(group = .draw), alpha = .2) +
  theme(
    axis.ticks = element_blank(), 
    axis.text = element_blank(), 
    axis.title = element_blank()
  ) + 
  expand_limits(y = 0:1) +
  ggtitle("Plausible curves before seeing data") + 
  ggdark::dark_theme_gray(base_size = 14)

# Maximum likelihood estimate
fm1 <- nls(prop ~ SSlogis(age, Asym, xmid, scal), data)
new_data <- tibble(age = 0:100) %>% 
  mutate(fit = predict(fm1, newdata = .))

p2 <- ggplot(data) + 
  aes(x = age, y = prop) + 
  geom_line(aes(y = fit), data = new_data, size = 1) +
  geom_point(color = "#cc0033", size = 4) + 
  theme(
    axis.ticks = element_blank(), 
    axis.text = element_blank(), 
    axis.title = element_blank()
  ) + 
  expand_limits(y = 0:1) +
  expand_limits(x = c(0, 100)) +
  ggtitle("How well do the curves fit the data") + 
  ggdark::dark_theme_gray(base_size = 14)


fit <- brm(
  model_formula,
  data = data,
  prior = c(prior_fixef, prior_phi),
  iter = 2000,
  chains = 4,
  cores = 1,
  control = list(adapt_delta = 0.9, max_treedepth = 15), 
  file = here("slides", "assets", "mods", "log_curve_mod")
)

draws_posterior <- data %>%
  tidyr::expand(age = 0:100) %>%
  tidybayes::add_fitted_draws(fit, n = 100) 

p3 <- ggplot(draws_posterior) +
  aes(x = age, y = .value) +
  geom_line(aes(group = .draw), alpha = .2) +
  geom_point(
    aes(y = prop), 
    color = "#cc0033", size = 4, 
    data = data
  ) +
  theme(
    axis.ticks = element_blank(), 
    axis.text = element_blank(), 
    axis.title = element_blank()
  ) +
  expand_limits(y = 0:1) +
  ggtitle("Plausible curves after seeing data") + 
  ggdark::dark_theme_gray(base_size = 14)

p1 + p2 + p3
```

---
class: middle, center
background-color: black

# .white["We put a tiny sliver of scientific information into the model and then we bet on the things that can happen a large number of ways"]

---
background-image: url(https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png)
background-size: contain

.footnote[[BDA walkthrough](./assets/bda_walkthrough/bda_walkthrough.zip)]

---
class: middle

<iframe src="https://allisonhorst.github.io/palmerpenguins/" style="border:none;" height="100%", width="100%">

---
exclude: true

`r AutoCite(bib, "mcelreath2018statistical")`

---
layout: false
class: title-slide-final, left

# References

```{r, 'refs', results='asis', echo=FALSE, eval=T, cache=FALSE}
PrintBibliography(bib)
```

[2] Bates, D. (2011). *Mixed models in R using the lme4 package Part 2: Longitudinal data, modeling interactions*. http://lme4.r-forge.r-project.org/slides/2011-03-16-Amsterdam/2Longitudinal.pdf

[3] Mahr, T. (2017). *Plotting partial pooling in mixed-effects models*. https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/

[4] Mahr, T. (2019). *Another mixed effects model visualization*. https://www.tjmahr.com/another-mixed-effects-model-visualization/

[5] Mahr, T. (2020) *Bayes’ theorem in three panels*. https://www.tjmahr.com/bayes-theorem-in-three-panels/

