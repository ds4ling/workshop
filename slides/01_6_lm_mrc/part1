class: inverse, middle

<blockquote align='center' class="twitter-tweet" data-lang="de">
<a href="https://twitter.com/AndrewsNotFunny/status/1365382679877812224"></a>
</blockquote>

---
class: title-slide-section-grey, middle

# Multiple regression and correlation

---
layout: true

# MRC

---

### Multiple Causation

- In nature: a phenomenon may have more than a single cause

- In statistics: criterion variable might have more than a single 
relevant predictor

- Leaving a potential cause out of the equation would constitute "omitting 
a relevant variable"
--

  - This biases the parameter estimates for the other predictors
  - We are not always sure of what those other predictors might be!

---

### Overview

- We have specified a linear formula that can account for the relationship 
between two continuous variables. 

.center[
.content-box-blue[
$Y = a + bX + e$
]
]

- It is uncommon for a given response variable to be determined by a single 
predictor.

- Most theories/models in social science predict complex relationships 
between multiple predictors.
  - Score ~ SES + IQ
  - Duration ~ speech rate + number of syllables
  - RT ~ working memory + word position

- We can extend the linear model equation to account for multiple predictors.

---

### Overview

.pull-left[

- We want to construct the equivalent of an **OR** statement or a *logical disjunction* 

- Boolean Algebra tells us that an **OR** statement implies *summing things*

- In other words, all X’s do not have to be high at once to have an effect on Y

- For example, you can get an overall high GRE score by scoring high in verbal 
but (somewhat) low in math:
  - They have to have a high total (or sum) 
  - A sum is implicitly an **OR** statement

]

.pull-right[

| A</br>(x) | B</br>(y) | C</br>(x **∨** y) |
| :-------: | :-------: | :---------------: |
| 0         | 0         |   **0**           | 
| 0         | 1         |   **1**           |
| 1         | 0         |   **1**           |
| 1         | 1         |   **1**           |

.center[

If A = 1  
**OR**  
B = 1,  
then C = 1  
Otherwise, C = 0

]
]

---
background-image: url(../../assets/img/pensar2.png)
background-position: 95% 50%

### Overview

- What if we based linguistics Grad School  
acceptances solely on GRE scores?

--

- One shouldn't have to score high on all of  
the entrance requirements, but just rank  
high on the sum of all of them

--

- We could use a weighted sum

---

### Overview

#### Weighted sum

- Some things are more important than others in this overall sum

- For example, GRE is actually a poor predictor of graduate student success

- So we might want to weight this variable lower than something 
  like GPA or letters of recommendation

- We can use least squares estimation to get the b-weights with 
the lowest SS<sub>ERROR</sub>

---

### The equation

.pull-left[
.content-box-blue[
$$Y = a + bX + e$$
]
]

--

.pull-right[
.content-box-red[
$$Y = a_{0} + b_{1}X_{1}  + b_{2}X_{2} {...} b_{k}X_{k} + e$$
]
]

--

### Least Squares Estimation

- R will estimate the b-weights that minimize the sum of the squared errors 
(`r emojifont::emoji("no_good_woman")` by hand examples)

- Ideally these are the b-weights that best represent how much each predictor 
is really contributing to the variance in the criterion variable (y)

- We will again use least squares estimation to achieve the best (optimal) 
regression weights

---

### Multiple predictors

- The multiple predictors represent different hypotheses regarding what 
might be affecting the criterion variable

- In other words, multiple regression is just creating a sum of weighted 
predictors to explain the total variance in the criterion variable

- The way that the predictors function together is not necessarily the 
same as the way that they each function alone

- Bivariate regression is just a degenerate form ("special case") of 
multiple regression containing only one predictor

---

### Interpretation

#### Summarizing so far...

- In multiple regression we assume additivity and linearity

- In Boolean Algebra, a sum (addition) represents a logical disjunction

- The multiple regression "weighted sum" is a complex **OR** statement

--

</br>

### What does this mean for our parameter estimates and how do we interpret them?

---

### (Semi)partial betas

- .RUred[Intercept] ( $a_{0}$ ): the value of the criterion variable when all 
predictors are equal to 0  
(same as bivariate regression)

- .RUred[Slope] ( $b_{k}$ ): the change in the criterion associated with a 
1-unit change in $X_k$... 
--
*with all other predictors held constant*

--

</br>

### How can we calculate the unique contribution of each predictor?

---
background-image: url(./assets/img/ballentine1.png)
background-size: contain
background-position: 95% 50%

---
background-image: url(./assets/img/ballentine2.png)
background-size: contain
background-position: 95% 50%

--

.pull-right[
.footnote[
Recall that the b-weight of the bivariate model is $r(\frac{s_y}{s_x})$
]
]

---
background-image: url(./assets/img/ballentine3.png)
background-size: contain
background-position: 95% 50%

---
background-image: url(./assets/img/ballentine4a.png)
background-size: contain
background-position: 95% 50%

### The "Ballantine"

.footnote[Cohen & Cohen, 1975]

---
background-image: url(./assets/img/ballentine4b.png)
background-size: contain
background-position: 95% 50%

- $r^{2}_{x1,x2} = c + d$

--

- $r^{2}_{y,x1} = a + c$

--

- $r^{2}_{y,x2} = b + c$

--

- $R^{2}_{y,x1,x2} = a + b + c$

---
background-image: url(./assets/img/ballentine5.png)
background-size: contain
background-position: 95% 50%

### Squared semipartial correlation

- $X_2$ removed from *explained*  
variance

--

- Represents unique contribution  
of $X_1$

--

- $sr^2 = R^{2}_{y,x1,x2} - r^{2}_{y,x2}$

---
background-image: url(./assets/img/ballentine7.png)
background-size: contain
background-position: 95% 50%

### Squared partial</br>correlation

- Y variance not accounted for by  
$X_2$ is the area of A + E

--

- The area explained by $X_1$ = A  
(squared semipartial correlation)

--

- Unqiue contribution of $X_1$ is  
$A / (A + E)$ or...

--

- $pr^{2}_{x1} = \frac{A}{A + E}$

--

- We are pretending that $X_2$  
doesn't exist (statistically)

---
background-image: url(./assets/img/ballentine4b.png)
background-size: 300px
background-position: 95% 20%

### Statistical control<sup>1</sup>

- Not the same as .green[experimental control]
  - We are interested in some treatment
  - Some participants receive treatment, some do not
  - *Only* looking at highly proficient bilinguals (not low, med. prof.)

- We partial out the effects of predictor $X_1$ from all other predictors and 
use this error to determine $b_1$

- We are estimating the effect of one predictor while holding the other 
predictors constant

- This will only work if predictors are not correlated (i.e., there *cannot* be 
multicollinearity)

.footnote[<sup>1</sup>This is a poor choice for a name. Ask why.]

---
background-image: url(https://i.imgflip.com/25dwia.jpg)
background-position: 90% 50%
background-size: 450px

### Conceptual understanding

.pull-left[

- MRC is much more difficult to conceptualize than the bivariate linear 
regression

- If we consider a simple three variable model (y ~ x<sub>1</sub> + 
x<sub>2</sub>) we are fitting a hyperplane to a three dimensional space

- More variables = more complexity

]

---

```{r, additive_plots, echo=FALSE, fig.width=14, fig.height=5}
p1 <- ggplot(mtcars, aes(x = wt, y = mpg)) + 
  geom_point() + 
  geom_smooth(method = lm, formula = "y ~ x") +
  ds4ling_bw_theme(base_family = "Times", base_size = 16)

p2 <- ggplot(mtcars, aes(x = drat, y = mpg)) + 
  geom_point() + 
  geom_smooth(method = lm, formula = "y ~ x") +
  ds4ling_bw_theme(base_family = "Times", base_size = 16)

p1 + p2
```

--

.center[
.RUred[mpg ~ wt] & .blue[mpg ~ drat]  
!=  
MRC
]

---
background-color: black

<iframe src="./assets/html/threejs1.html" width='1000' height='500' style="border:none;"></iframe>

---

```{r, additive_3d, echo=FALSE}

# Left and right sides
plot3D::scatter3D(x, y, z, 
    pch = 21, cex = 1, expand = 0.75, colkey = F,
    theta = 0, phi = 5, ticktype = "detailed",
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    col.panel ="steelblue")

plot3D::scatter3D(x, y, z, 
    pch = 21, cex = 1,  expand = 0.75, colkey = F,
    theta = 90, phi = 5, ticktype = "detailed",
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    col.panel ="steelblue")
```

---

```{r, additive_3d_surface, echo=FALSE}
# Left and right sides with regression surface
plot3D::scatter3D(x, y, z, 
    pch = 21, cex = 1, expand = 0.75, colkey = F,
    theta = 0, phi = 5, ticktype = "detailed",
    xlab = "wt", ylab = "drat", zlab = "mpg",
    surf = list(x = x_pred, y = y_pred, z = z_pred_add,  
                facets = NA, col = 'grey60'))

plot3D::scatter3D(x, y, z, 
    pch = 21, cex = 1,  expand = 0.75, colkey = F,
    theta = 90, phi = 5, ticktype = "detailed",
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_add,  
                facets = NA, col = 'grey60'))
```

---
background-image: url(./assets/img/additive_3d_corner.png)
background-size: contain

---
layout: true

# MRC

---

### Doing it in R

.pull-left[

```{r, fit_mrc, echo=FALSE}
mod <- lm(mpg ~ wt + drat, data = mtcars)
summary(mod)
```

]

.pull-right[

```{r, fit_mrc_fake, eval=FALSE}
mod <- lm(mpg ~ wt + drat, data = mtcars) #<<
summary(mod)
```

]

---

### CIs and significance tests

```{r, fit_mrc_print, echo=FALSE}
cis <- confint(mod)
summary(mod) %>% 
  tidy(.) %>% 
  mutate(., `95% LB` = cis[, 1], `95% UB` = cis[, 2]) %>%
  dplyr::select(., ` ` = term, beta = estimate, SE = std.error, 
                `95% LB`, `95% UB`, `t-value` = statistic, p.value) %>% 
  kable(., format = 'html', digits = 2) %>% 
  kable_styling(., font_size = 22)
```

</br>

- Same as bivariate case, but we adjust t-value for k (added estimated 
parameters)

--

- Statistical significance implies that the 95% CI doesn't contain 0

--

- **Rule of thumb**: multiply SE of b-weight by 2 and add/subtract to/from 
b-weight  

--

- T-ratio: Parameter estimate divided by SE  

--
(i.e., 30.29 / 7.32 = `r round(30.29 / 7.32, 2)`)

--

- **Rule of thumb**: |t| > 2 = significant<sup>&trade;</sup> 

---

### Coefficient of multiple determination: R<sup>2</sup>

- Adding variables will always explain more variance

- Not necessarily better

- There is an adjustment for exhausting degrees of freedom

--

### Note

- .green[r]: pearson product moment correlation

- .purple[r<sup>2</sup>]: coefficient of determination; variance explained 
(bivariate case)

- .RUred[R<sup>2</sup>]: coefficient of multiple determination; variance 
explained (MRC)

---

### Making predictions

- Recall the multiple regression equation...

$$Y = a_{0} + b_{1}X_{1}  + b_{2}X_{2} {...} b_{k}X_{k} + e$$

- Our `mtcars` model can be summarized as...

```{r, echo=F}
extract_eq(mod, wrap = T, use_coefs = T, fix_signs = F, ital_vars = T)
```

--

- What is the predicted `mpg` for a car that weighs 1 unit with 
a rear axel ratio (drat) of 2?  
And one that weighs 1 with a drat of 4?  
And one that weighs 3 with a drat of 2.5?

--

  - `r round(coef(mod)[1] + (1 * coef(mod)[2]) + (2 * coef(mod)[3]), 2)` 
  $mpg = 30.29 + -4.78 \times 1 + 1.44 \times 2$

  - `r round(coef(mod)[1] + (1 * coef(mod)[2]) + (4 * coef(mod)[3]), 2)` 
  $mpg = 30.29 + -4.78 \times 1 + 1.44 \times 4$

  - `r round(coef(mod)[1] + (3 * coef(mod)[2]) + (2.5 * coef(mod)[3]), 2)` 
  $mpg = 30.29 + -4.78 \times 3 + 1.44 \times 2.5$

---
layout: false
class: title-slide-section-grey, middle

# Interactions

---
layout: true

# Interactions

---

#### Recall

.pull-left[

- Assumptions of Multiple Regression: 
  - Additivity
  - Linearity
- In Boolean Algebra, a sum (addition) represents a logical disjunction: 
  - Multiple regression "weighted sum" is a complex "OR" statement

]

.pull-right[

| A</br>(x) | B</br>(y) | C</br>(x **∨** y) |
| :-------: | :-------: | :---------------: |
| 0         | 0         |   **0**           | 
| 0         | 1         |   **1**           |
| 1         | 0         |   **1**           |
| 1         | 1         |   **1**           |

.center[

If A = 1  
**OR**  
B = 1,  
then C = 1  
Otherwise, C = 0

]
]

---

#### Recall

.pull-left[

- Assumptions of Multiple Regression: 
  - Additivity
  - Linearity
- In Boolean Algebra, a sum (addition) represents a logical .blue[disjunction]: 
  - Multiple regression "weighted sum" is a complex "OR" statement

#### Non-Additivity: Interaction Terms

- In Boolean Algebra, a product (multiplication) represents a logical 
**conjunction**:
  - Interaction terms represent "AND" terms
  - These are included within the overall "OR" statement

]

.pull-right[

| A</br>(x) | B</br>(y) | C</br>(x **∧** y) | </br>.grey[(x ∨ y)] |
| :-------: | :-------: | :---------------: | :-----------------: |
| 0         | 0         |   **0**           |   .lightgrey[0]     |
| 0         | 1         |   **0**           |   .lightgrey[1]     |
| 1         | 0         |   **0**           |   .lightgrey[1]     |
| 1         | 1         |   **1**           |   .lightgrey[1]     |

.center[

If A = 1  
**AND**  
B = 1,  
then C = 1  
Otherwise, C = 0

]
]

---

.pull-left[

### Genetics example

- You share 50% of genes with your Mom (M) and 50% with your Dad (D)

- But your parents don’t share that many genes

- M and D are generally not genetically correlated with each other, but you 
(the M\*D interaction) are correlated with both M and D (more on this later)

]

--

.pull-right[

### Drugs and alcohol

Consider taking the upcoming midterm in one of the following conditions

- Neither Drugs nor Alcohol:
  - probably best, produce highest score
- Alcohol alone:
  - Probably lower midterm scores than doing neither
- Drugs alone:
  - Probably lower midterm scores than doing neither
- Alcohol and Drugs together:
  - Probably lowest scores on the midterm exam of 
    all the four possible conditions

]

---

### Drugs and alcohol continued

- Is the effect of alcohol and drugs together equal to the negative effect 
of alcohol PLUS the negative effect of drugs?
  - Probably not
  - Alcohol and drugs are known to interact 

- This effect is a negative interaction

- If you mix alcohol and drugs they have a larger combined effect than 
adding up the effects of each acting separately:
  - For example, if alcohol makes your score drop 10 points, and the drug 
  drops it 15 points, when you take both your score will very likely drop 
  more than just 25 points

- This effect is not additive, it is multiplicative

---

### Non-additivity of effects

- You cannot add the effect of alcohol to the effect of drugs to predict the effect "alcohol AND drugs"

- Because you get an extra effect (a boosting effect) by combing them

- There is a synergistic effect of combining the terms

- In principle, could be either more or less effective

- You include this multiplicative **AND** term (A\*D) in *ADDITION* to the other terms in the model

---

.pull-left[

### The multiple regression formula: 

$$Y = a_{0} + b_{1}X_{1} + b_{2}X_{2} + (b_{1}X_{1} \times b_{2}X_{2}) + e$$

### Including interactions in R

- We do this using `:`

- Or \*

]

--

.pull-right[

```{r, nonadditive_mod}
m <- lm(mpg ~ wt + drat + wt:drat, data = mtcars) #<<
summary(m)
```

]

---

### Visualization

- Including an interaction affects the hyperplane fit to the data

---
background-color: black

```{r additive_interaction, echo=FALSE}
# compare additive and int models
par(bg = 'black')
plot3D::scatter3D(x, y, z, 
    pch = 1, cex = 0, bty = "bl", expand = 0.75,
    theta = 50, phi = 10,  colkey = F, tick.col = 'white', 
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_add,  
                facets = NA))

plot3D::scatter3D(x, y, z, 
    pch = 1, cex = 0, bty = "bl", expand = 0.75,
    theta = 50, phi = 10,  colkey = F,
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
                facets = NA))
```

---

```{r additive_interaction2, echo=FALSE}
# compare additive and int models
#par(bg = 'black')
plot3D::scatter3D(x, y, z, 
    pch = 1, cex = 0, bty = "bl", expand = 0.75,
    theta = 50, phi = 10,  colkey = F,
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_add,  
                facets = NA))

plot3D::scatter3D(x, y, z, 
    pch = 1, cex = 0, bty = "bl", expand = 0.75,
    theta = 50, phi = 10,  colkey = F,
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
                facets = NA))
```

---
background-color: black

```{r additive_interaction3, echo=FALSE}
# compare additive and int models
par(bg = 'black')
plot3D::scatter3D(x, y, z, 
    pch = 1, cex = 0, bty = "bl", expand = 0.75,
    theta = 0, phi = 5,  colkey = F,
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
                facets = NA))

plot3D::scatter3D(x, y, z, 
    pch = 1, cex = 0, bty = "bl", expand = 0.75,
    theta = 90, phi = 5,  colkey = F,
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
                facets = NA))
```

---

```{r additive_interaction4, echo=FALSE}
# compare additive and int models
plot3D::scatter3D(x, y, z, 
    pch = 1, cex = 0, bty = "bl", expand = 0.75,
    theta = 0, phi = 5,  colkey = F,
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
                facets = NA))

plot3D::scatter3D(x, y, z, 
    pch = 1, cex = 0, bty = "bl", expand = 0.75,
    theta = 90, phi = 5,  colkey = F,
    xlab = "wt", ylab = "drat", zlab = "mpg", 
    surf = list(x = x_pred, y = y_pred, z = z_pred_int,  
                facets = NA))
```

---
layout: false
class: title-slide-section-red

# Review

### What we've seen so far

- Bivariate regression
- MRC
- Additive effects
- Interactions (multiplicative effects)

### What's left

- Assumptions
- Model specification
- Alpha slippage
- Empirical selection of variables
- Reporting results
